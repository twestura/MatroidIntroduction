\documentclass[twoside]{article}

\usepackage{mystyle}
\usepackage{tikz}

\newcommand{\I}{\mathcal{I}}

\title{Introduction to Matroids}
\author{Travis Westura}
\date{\today}

\begin{document}
\maketitle
\thispagestyle{empty}

\section{Introduction}

In this course we've seen several examples of algorithms that are ``greedy.''
A greedy algorithm is an algorithm that makes a ``locally best'' decision.
For example, in Dijkstra's algorithm we use a priority queue to keep track of nodes on the friontier and pop them off using the priority of shortest path distance.
And in Kruskal's algorithm for finding minimum weight spanning trees, at each step we select the minimum weight edge that does not form a cycle.

We can generalize the idea of ``locally make a best decision'' by using matroids.
The word \emph{matroid} should make you think of the word \emph{matrix}, which you use in linear algebra and multivariable or vector calculus.
We'll begin by reviewing the concepts of linear independence, then we'll discuss analogous concepts in graph theory.
Then we will relate these concepts by generalizing them and giving the definition of a matroid.
And finally we will use matroids to give a proof of the correctness of Kruskal's algorithm.

\section{Independence in Linear Algebra}

As you take Linear Algebra and Multivariable Calculus you will gain lots of experience working with vectors and matrices.
In these notes we'll denote vectors using boldface letters at the end of the alphabet, such as $\bv{u}$ and $\bv{v}$, and matrices using capital letters at the beginning of the alphabet, such as $A$ and $B$.
We'll denote by $\bv{0}$ the vector of all $0$'s, and we'll also write vectors as columns and matrices as rectangles containing numbers:
\begin{equation*}
  \bv{u} = \mat{3\\-1\\3}, \quad \bv{v} = \mat{v_1\\v_2\\v_3}, \quad \bv{0} = \mat{0\\0\\0}, \quad A = \mat{1 & 2 & 5\\-2 & 3 & 0}, \quad B = \mat{b_{1,1} & b_{1, 2} & b_{1, 3}\\b_{2, 1} & b_{2, 2} & b_{2, 3}}.
\end{equation*}
We say that vectors are elements of a set called a Vector Space, and we also have the ability to add vectors and to multiply them by scalars.
In multivariable calculus the most common example of vector spaces are $\R^2$ and $\R^3$, where vectors consist of tuples of $2$ and $3$ numbers, respectively.
We can add vectors and multiply them by scalars as follows:
\begin{equation*}
  \mat{u_1\\u_2} + \mat{v_2\\v_2} = \mat{u_1 + v_1\\v_2 + v_2}, \quad a\mat{u_1\\u_2} = \mat{a u_1\\ a u_2}.
\end{equation*}
A linear combination of vectors is a sum of scalar multiples of vectors.
For example
\begin{equation*}
  2\mat{2\\-1} + 3\mat{-3\\4} = \mat{-5\\10}.
\end{equation*}
We could say that the $3$rd vector depends on the first two vectors, since it is a linear combination of them.

A set of vectors\footnote{
  We can also treat the columns of matrices as vectors and define a notion of linear independence on them, although we should be careful that a column may be repeated, whereas a vector in a set is not repeated.
}
$\{\bv{v}_1, \bv{v}_2, \ldots, \bv{v}_n\}$ is \emph{linearly independent} if the only scalars ${a_1, a_2, \ldots, a_n}$ that satisfy
\begin{equation*}
  a_1\bv{v}_1 + a_2\bv{v}_2 + \cdots + a_n\bv{v}_n = \bv{0}
\end{equation*}
are ${a_1 = a_2 = \cdots = a_n = 0}$.This means there is no way to write one of the vectors as a combination of the others, unless we make all the coefficients~$0$.
That is, none of the vectors in the set depend on the others.
For example, the standard basis vectors in $\R^3$ are linearly independent,
\begin{equation*}
  \left\{\mat{1\\0\\0}, \mat{0\\1\\0}, \mat{0\\0\\1}\right\},
\end{equation*}
as none of them can be written as a linear combination of the others.
The following set of vectors is linearly dependent:
\begin{equation*}
  \left\{\mat{1\\2}, \mat{3\\2}, \mat{5\\6}\right\}, \quad 2\mat{1\\2} + \mat{3\\2} = \mat{2 + 3\\4 + 2} = \mat{5\\6}.
\end{equation*}

There is a limit to the number of vectors that we can add to a set while maintaining independence.
For example, in~$\R^3$, and set of $4$~vectors is linearly dependent.
An independent set to which we can't add any more vectors is called a \emph{maximal} independent set, with the word maximal meaning we have included as many vectors as possible.\footnote{
  A maximal linearly independent set of vectors is called a \emph{basis}.
}

Further, if we have an independent set of vectors, we can remove vectors and still have an independent set.
For example, the subset of standard basis vectors,
\begin{equation*}
  \left\{\mat{1\\0\\0}, \mat{0\\1\\0}\right\},
\end{equation*}
is still independent in~$\R^3$.

\section{Independence in Graphs}

Now let's discuss independence in graphs.
Recall that a graph~${G = (V, E)}$ consists of a set~$V$ of vertices and a set~$E$ of edges.
We will consider only undirected graphs for now.

Recall that a \emph{tree} is a connected graph that does not have any cycles.
A \emph{forest} is a graph in which each connected component is a tree.
That is, a forest is essentially a collection of trees grouped together.
A \emph{spanning tree} of an undirected\footnote{ There is also a notion of spanning tree in an undirected graph, called an \emph{arborescence}.} graph~$G$ is a subgraph that is a tree and that includes all of the vertices of~$G$.
The ``span'' part of a spanning tree refers to the tree containing all of the vertices.
In the following diagram, from left to right we have: a tree that does not span the graph, a cycle, two spanning trees, and a forest consisting of two trees.
\begin{center}
  \begin{tikzpicture}[every node/.style={circle,thick,draw}]
    \node (A) at (0, 0) {A};
    \node (B) at (1.5, 0) {B};
    \node (C) at (0, -1.5) {C};
    \node (D) at (1.5, -1.5) {D};

    \path[-] (A) edge[draw=gray,thin] (B);
    \path[-] (A) edge[draw=black,very thick] (C);
    \path[-] (B) edge[draw=gray,thin] (D);
    \path[-] (C) edge[draw=black,very thick] (D);
    \path[-] (A) edge[draw=gray,thin] (D);
  \end{tikzpicture}
  \quad
  \begin{tikzpicture}[every node/.style={circle,thick,draw}]
    \node (A) at (0, 0) {A};
    \node (B) at (1.5, 0) {B};
    \node (C) at (0, -1.5) {C};
    \node (D) at (1.5, -1.5) {D};

    \path[-] (A) edge[draw=gray,thin] (B);
    \path[-] (A) edge[draw=black,very thick] (C);
    \path[-] (B) edge[draw=gray,thin] (D);
    \path[-] (C) edge[draw=black,very thick] (D);
    \path[-] (A) edge[draw=black,very thick] (D);
  \end{tikzpicture}
  \quad
  \begin{tikzpicture}[every node/.style={circle,thick,draw}]
    \node (A) at (0, 0) {A};
    \node (B) at (1.5, 0) {B};
    \node (C) at (0, -1.5) {C};
    \node (D) at (1.5, -1.5) {D};

    \path[-] (A) edge[draw=gray,thin] (B);
    \path[-] (A) edge[draw=black,very thick] (C);
    \path[-] (B) edge[draw=black,very thick] (D);
    \path[-] (C) edge[draw=black,very thick] (D);
    \path[-] (A) edge[draw=gray,thin] (D);
  \end{tikzpicture}
  \quad
  \begin{tikzpicture}[every node/.style={circle,thick,draw}]
    \node (A) at (0, 0) {A};
    \node (B) at (1.5, 0) {B};
    \node (C) at (0, -1.5) {C};
    \node (D) at (1.5, -1.5) {D};

    \path[-] (A) edge[draw=gray,thin] (B);
    \path[-] (A) edge[draw=black,very thick] (C);
    \path[-] (B) edge[draw=black,very thick] (D);
    \path[-] (C) edge[draw=gray,thin] (D);
    \path[-] (A) edge[draw=black,very thick] (D);
  \end{tikzpicture}
  \quad
  \begin{tikzpicture}[every node/.style={circle,thick,draw}]
    \node (A) at (0, 0) {A};
    \node (B) at (1.5, 0) {B};
    \node (C) at (0, -1.5) {C};
    \node (D) at (1.5, -1.5) {D};
    \node (E) at (3, 0) {E};
    \node (F) at (3, -1.5) {F};

    \path[-] (A) edge[draw=gray,thin] (B);
    \path[-] (A) edge[draw=black,very thick] (C);
    \path[-] (B) edge[draw=gray,thin] (D);
    \path[-] (C) edge[draw=gray,thin] (D);
    \path[-] (A) edge[draw=black,very thick] (D);
    \path[-] (B) edge[draw=black,very thick] (E);
    \path[-] (E) edge[draw=black,very thick] (F);
    \path[-] (D) edge[draw=gray,thin] (F);
  \end{tikzpicture}
\end{center}
The independent sets of a graph are the forests, that is, the subgraphs that do not include cycles.
The dependent sets are the subgraphs that include cycles.
To see the analogy to linear algebra, here note that if we have a forest that is not a spanning tree, then there is still an edge we can add to it to obtain a larger subggraph without creating a cycle.
A spanning tree gives us a notion of maximal independent set, meaning that we have included as many edges as we possibly can.

And given a forest, removing edges does not create a cycle.
Thus we have the same property that removing elements from an independent set preserves independence.

\section{Definition of Matroids}

Now that we have some intuition about the concept of ``independence,'' let's give a definition of matroids.\footnote{
  We could define matroids in many ways.
  Matroids are referred to as \emph{cryptomorphic}, which means they have many equivalent but ostensibly unrelated definitions.}
Many mathematical definitions involve placing a set inside of parentheses with other sets or functions that further describe that set.
For example, a graph is given by two sets ${G = (V, E)}$.
A vector space is given by ${(V, k, +, \cdot)}$, where $V$ is a set of vectors, $k$ is a field of scalars, $+$ is an addition of vectors, and $\cdot$ is a multiplication between a scalar and a vector.
A familiar example is ${(\R^3, \R, +, \cdot)}$.
If you have taken CS~$2800$ or other more advanced courses, you will have seen groups, where are often described by writing ${(G, +)}$, where $G$ is a set and $+$ is a binary operation on that set.
Matroids have a similar definition, where we have a set that we write in parentheses together with some other information describing that set.
In this definition the additional information is a collection of subsets.

\begin{defn}[Matroid]
  A \emph{matroid}~$M$ is a pair~$(E, \I)$ consisting of a finite set~$E$ and a collection~$\I$ of subsets of~$E$, called the \emph{independent sets}, satisfying the following properties:
  \begin{enumerate}
    \item The collection is nonempty.
      That is, ${\I \ne \varnothing}$.
    \item All subsets of an independent set are independent.
      That is, if ${A \in \I}$ and ${B \subseteq A}$, then ${B \in \I}$.
    \item If $A$ and $B$ are independent sets and $A$ is larger than $B$, then we can take an element that is in $A$ but not in $B$ and add it to $B$ to construct an independent set.
    That is, if ${A, B \in \I}$ with ${|A| > |B|}$, then there exists ${x \in A \setminus B}$ such that ${B \cup \{x\} \in \I}$.
    This property is knows as the \emph{independent set exchange property}---we can ``exchange'' an element from one set to another.
  \end{enumerate}
  A subset of~$E$ that is not independent is called \emph{dependent}.
\end{defn}

Let's see how this definition fits in with our first two examples.
Recalling that a vector space has a set of vectors~$V$, we form a matroid~$(V, \I)$, where $\I$ is the collection of independent sets of vectors of~$V$.
Recall what we noted previously: given a set of independent vectors, removing vectors still results in an independent set.
And if there are two independent sets of vectors ${A = \{\bv{a}_1, \ldots, \bv{a}_n\}}$ and ${B = \{\bv{b}_1, \ldots, \bv{b}_m\}}$ with ${m < n}$, then clearly $B$ is not a maximal independent set, so we can add more vectors to it while maintaining independence.\footnote{
  Being formal about explicitly satisfying the thrid property requires slightly more linear algebra than I want to use right now, so I'll leave this verification as an exercise for those interested.
}

Next, given an undirected graph ${G = (V, E)}$, we form a matroid~$(E, \I)$ with the ground set given by the graph's edges and the independent sets given by the graph's forests.
Let's verify each part of the definition.
\begin{enumerate}
  \item The empty collection of edges forms a forest vacuously.
  \item If a graph does not form a cycle, then removing edges cannot yield a cycle.
  \item Consider two forests $A$ and $B$ where $A$ has more edges than $B$.
    Then there is some edge ${e \in A}$ such that ${B \cup \{e\}}$ does not have a cycle.
    The number of connected components in a graph ${(V, F)}$ is given by ${|V| - |F|}$.
    Note that individual vertices form their own component.
    If $A$ and $B$ are two forests with ${|A| > |B|}$, then there are more connected components in ${(V, B)}$ than in ${(V, A)}$.
    There there is some edge~$e$ in~$A$ that connects two of the components in~$B$.
    Thus ${B \cup \{e\}}$ forms a forest with one more edge than~$B$.
\end{enumerate}

\section{Kruskal's Algorithm and Matroid Optimization}

Given a weighted undirected graph, we construct a spanning tree by beginning by starting with the empty set and repeatedly adding the minimum weight edge that does not form a cycle.
That is, at each step, add the minimum weight edge that still produces an independent set.
The algorithm terminates when we have a spanning tree, or in the language of matroids, when we have a maximal independent set.

In the following example we start with the empty set of edges.
The minimum weight edge is between~$A$ and~$C$, the we first choose that edge.
Next we select the edge between~$A$ and~$D$.
We then can't select the edge between~$C$ and~$D$, as that edge would form a cycle.
Thus we select the edge between~$B$ and~$D$.
At this point there are no other edges we can add without creating a cycle, so we have a spanning tree and terminate out algorithm here.
\begin{center}
  \begin{tikzpicture}
    \node[shape=circle,draw=black,thick] (A) at (0, 0) {A};
    \node[shape=circle,draw=black,thick] (B) at (1.5, 0) {B};
    \node[shape=circle,draw=black,thick] (C) at (0, -1.5) {C};
    \node[shape=circle,draw=black,thick] (D) at (1.5, -1.5) {D};

    \path[-] (A) edge[draw=gray,thin] node[above] {5} (B);
    \path[-] (A) edge[draw=gray,thin] node[left] {1} (C);
    \path[-] (B) edge[draw=gray,thin] node[right] {4} (D);
    \path[-] (C) edge[draw=gray,thin] node[below] {3} (D);
    \path[-] (A) edge[draw=gray,thin] node[above right] {2} (D);
  \end{tikzpicture}
  \quad
  \begin{tikzpicture}
    \node[shape=circle,draw=black,thick] (A) at (0, 0) {A};
    \node[shape=circle,draw=black,thick] (B) at (1.5, 0) {B};
    \node[shape=circle,draw=black,thick] (C) at (0, -1.5) {C};
    \node[shape=circle,draw=black,thick] (D) at (1.5, -1.5) {D};

    \path[-] (A) edge[draw=gray,thin] node[above] {5} (B);
    \path[-] (A) edge[draw=black,very thick] node[left] {1} (C);
    \path[-] (B) edge[draw=gray,thin] node[right] {4} (D);
    \path[-] (C) edge[draw=gray,thin] node[below] {3} (D);
    \path[-] (A) edge[draw=gray,thin] node[above right] {2} (D);
  \end{tikzpicture}
  \quad
  \begin{tikzpicture}
    \node[shape=circle,draw=black,thick] (A) at (0, 0) {A};
    \node[shape=circle,draw=black,thick] (B) at (1.5, 0) {B};
    \node[shape=circle,draw=black,thick] (C) at (0, -1.5) {C};
    \node[shape=circle,draw=black,thick] (D) at (1.5, -1.5) {D};

    \path[-] (A) edge[draw=gray,thin] node[above] {5} (B);
    \path[-] (A) edge[draw=black,very thick] node[left] {1} (C);
    \path[-] (B) edge[draw=gray,thin] node[right] {4} (D);
    \path[-] (C) edge[draw=gray,thin] node[below] {3} (D);
    \path[-] (A) edge[draw=black,very thick] node[above right] {2} (D);
  \end{tikzpicture}
  \quad
  \begin{tikzpicture}
    \node[shape=circle,draw=black,thick] (A) at (0, 0) {A};
    \node[shape=circle,draw=black,thick] (B) at (1.5, 0) {B};
    \node[shape=circle,draw=black,thick] (C) at (0, -1.5) {C};
    \node[shape=circle,draw=black,thick] (D) at (1.5, -1.5) {D};

    \path[-] (A) edge[draw=gray,thin] node[above] {5} (B);
    \path[-] (A) edge[draw=black,very thick] node[left] {1} (C);
    \path[-] (B) edge[draw=black,very thick] node[right] {4} (D);
    \path[-] (C) edge[draw=gray,thin] node[below] {3} (D);
    \path[-] (A) edge[draw=black,very thick] node[above right] {2} (D);
  \end{tikzpicture}
\end{center}

We are interested in proving the correctness of this abstract algorithm.
For now we don't consider the implementation details.\footnote{
  For the details of an efficient implementation, look up the Union-Find data structure.
}
Kruskal's spanning tree algorithm is actually a case of the more general greedy algorithm that is used for weighted matroids.
A \emph{weighted matroid} is a matroid in which every element of the ground set has a nonnegative weight.
That is, a weighted matroid~${(E, \I)}$ has a weight~${w_e \ge 0}$ for each edge~${e \in E}$.
Finding a minimum weight spanning tree then becomes the problem of finding a minmum weight maximal independent set.\footnote{
  To use more terminalogy from linear algebra, a maximal independent set is called a \emph{base}.
  All maximal independent sets contain the same number of elements, and this number is called the \emph{rank} of the matroid.
}

In this general case the greedy algorithm works the same way.
Begin with the empty set.
Repeated select the minimum weight element of~$E$ that maintains independence.
Terminate when the only remaining elements of~$E$ would form a dependent set if added.

\begin{thm}[]
  Let ${(E, \I)}$ be a matroid with weights ${w_e \ge 0}$ for each~${e \in E}$.
  Then the greedy algorithm produces a maximal independent set of minimum weight.
\end{thm}
\begin{proof}
  Suppose that the greedy algorithm selects elements ${[e_1, e_2, \ldots , e_r]}$, in that order.
  These elements form an independent set, and their weights are in ascending order: ${w_{e_1} \le w_{e_2} \le \cdots \le w_{e_r}}$.
  To see this set is of minimum weight, suppose there is another maximal independent set consisting of ${[x_1, \ldots, x_r]}$ of lower weight.
  Then for some $i$ we must have ${w_{x_i} \le w_{e_i}}$.
  Use the least such $i$, which we note must be greater than~$1$, since the greedy algorithm picks the min cost edge at the first step.
  Now consider ${B = \{e_1, \ldots, e_{i-1}\}}$ and ${A = \{x_1, \ldots, x_i\}}$.
  Since ${|A| > |B|}$ and both sets are independent, we use the exchange property.
  There is some $j$ such that ${x_j \in A \setminus B}$ and ${B \cup \{x_j\}}$ is independent.
  But since ${w_{x_j} \le w_{x_i} < w_{e_i}}}$, we have a contradiction, as the greedy algorithm would have picked $x_j$ before $e_i$.
\end{proof}

There is also a converse to the previous result: if an independence system ${(E, \I)}$ is not a matroid, then the greedy algorithm fails for some choice of integer weights.
You'll learn more about Greedy Algorithms if you take the undergraduate algorithms course CS~$4820$, and you'll learn more about matroids if you take the graduate algorithms course CS~$6820$ or the combinatorics course sequence~Math~$4410$ and~$4420$.

\end{document}
